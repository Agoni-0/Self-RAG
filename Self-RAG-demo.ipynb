{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afaae1e-1e59-4faa-ae6c-97088f38fcc8",
   "metadata": {},
   "source": [
    "LLM模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d126c9f1-aca7-449d-8c07-e4787524b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_IsUSE = 1\n",
    "w_IsSUP = 1\n",
    "w_IsREL = 1\n",
    "use_IsUSE = True\n",
    "use_IsSUP = True\n",
    "use_IsREL = True\n",
    "show_details = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49681fe1-312e-413e-bd93-c0084d4824ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: self-rag/retrieval_lm/contriever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at self-rag/retrieval_lm/contriever were not used when initializing Contriever: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing passages from files ['self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_dec_2020_contriever_intro/passages_00', 'self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_dec_2020_contriever_intro/passages_01', 'self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_dec_2020_contriever_intro/passages_02', 'self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_dec_2020_contriever_intro/passages_03']\n",
      "Loading file self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_dec_2020_contriever_intro/passages_00\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# 添加了新的查询路径\n",
    "sys.path.append(\"self-rag/retrieval_lm/\")\n",
    "from passage_retrieval import Retriever\n",
    "import numpy as np\n",
    "\n",
    "class Retriever_LLM:\n",
    "    def __init__(self, top_k:int = 5):\n",
    "        self.retriever = Retriever({})\n",
    "        self.retriever.setup_retriever_demo(\"self-rag/retrieval_lm/contriever\", \"self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_2020_dec_intro_only.jsonl\", \"self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_dec_2020_contriever_intro/*\",  n_docs=5, save_or_load_index=False)\n",
    "    def search(self, query, n_docs:int = 5):\n",
    "        retrieved_documents = self.retriever.search_document_demo(query, n_docs)\n",
    "        return retrieved_documents\n",
    "\n",
    "retriever = Retriever_LLM(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1fe759-8e8f-4aa2-ad2d-534f938247ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response_LLM:\n",
    "    def __init__(self, token_ids, text, logprobs):\n",
    "        self.token_ids = token_ids\n",
    "        self.text = text\n",
    "        self.logprobs = logprobs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbfff7-983a-4735-865a-c6162a602aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from utils import load_special_tokens\n",
    "\n",
    "class Model_LLM:\n",
    "    def __init__(self, local_model_path: str, max_tokens: int, skip_special_tokens: bool, logprobs: int, tokenizer):\n",
    "        self.model = LLM(model=local_model_path, dtype=\"half\")\n",
    "        self.sampling_params = SamplingParams(temperature=0.0, \n",
    "                                              top_p=1.0, top_k = -1, max_tokens=max_tokens, skip_special_tokens=skip_special_tokens, \n",
    "                                              logprobs = logprobs, stop = [\"[Retrieval]\"])\n",
    "        self.ret_tokens, self.rel_tokens, self.sup_tokens, self.use_tokens = load_special_tokens(\n",
    "        tokenizer, use_grounding=use_IsSUP, use_utility=use_IsUSE)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    # 判断是否需要进行检索\n",
    "    # params\n",
    "    # response:Response_LLM 模型的输出\n",
    "    # return\n",
    "    # bool 是否需要检索\n",
    "    def need_retrieve(self, response: Response_LLM):\n",
    "        if 32001 in response.token_ids:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # 生成规范查询语句\n",
    "    # params\n",
    "    # input:str 输入\n",
    "    # paragraph:str 查询内容\n",
    "    # return\n",
    "    # str:规范的prompt\n",
    "    def format_prompt(self, input, paragraph=None):\n",
    "        prompt = \"### Instruction:\\n{0}\\n\\n### Response:\\n\".format(input)\n",
    "        if paragraph is not None:\n",
    "            prompt += \"[Retrieval]<paragraph>{0}</paragraph>\".format(paragraph)\n",
    "        return prompt\n",
    "        \n",
    "    # 询问大模型（底层）\n",
    "    # params\n",
    "    # query: str 模型输入\n",
    "    # return\n",
    "    # Response_LLM 模型的输出\n",
    "    def query_llm(self, query):\n",
    "        prompt = [query]\n",
    "        preds = self.model.generate(prompt, self.sampling_params)\n",
    "        pred_token_ids = preds[0].outputs[0].token_ids\n",
    "        pred_text = preds[0].outputs[0].text\n",
    "        pred_log_probs = []\n",
    "        for logprob in preds[0].outputs[0].logprobs:\n",
    "            tmp_log_probs = {}\n",
    "            for key, value in logprob.items():\n",
    "                tmp_log_probs[key] = np.exp(float(value))\n",
    "            pred_log_probs.append(tmp_log_probs)\n",
    "        response =  Response_LLM(pred_token_ids, pred_text, pred_log_probs)\n",
    "        return response\n",
    "    # 需要查询时询问大模型\n",
    "    # params\n",
    "    # prompt:str 之前的查询的问题+之前生成的文本\n",
    "    # document:list[str] Retrieval返回的top-k个文本\n",
    "    # return\n",
    "    # Response_LLM:所有生成结果中最好的\n",
    "    def re_query(self, prompt, documents):\n",
    "        max_score = 0\n",
    "        best_response = None\n",
    "        for document in documents:\n",
    "            response = self.query_llm(self.format_prompt(prompt, document))\n",
    "            cur_score = self.eval_generation(response)\n",
    "            if show_details:\n",
    "                print(\"此次检索的结果如下\")\n",
    "                print(document)\n",
    "                print(cur_score)\n",
    "                print(response.token_ids)\n",
    "                print(response.text)\n",
    "            if (cur_score > max_score):\n",
    "                max_score = cur_score\n",
    "                best_response = response\n",
    "        return best_response\n",
    "    # 根据token评估查询结果\n",
    "    # params\n",
    "    # response:Response_LLM 模型输出\n",
    "    # return\n",
    "    # float 输出评分\n",
    "    def eval_generation(self, response:Response_LLM):\n",
    "        sup_score = 0.0\n",
    "        if self.sup_tokens is not None:\n",
    "            num = 0\n",
    "            for tok_idx, tok in enumerate(response.token_ids):\n",
    "                if tok in list(self.sup_tokens.values()):\n",
    "                    token = self.tokenizer.convert_ids_to_tokens(tok)\n",
    "                    if token == \"[Fully supported]\":\n",
    "                        sup_score += response.logprobs[tok_idx][tok] * 0.5 + 0.5\n",
    "                    if token == \"[Partially supported]\":\n",
    "                        sup_score += response.logprobs[tok_idx][tok] * 0.5\n",
    "                    num += 1\n",
    "            if num != 0:\n",
    "                sup_score /= num\n",
    "            else:\n",
    "                sup_score = 0.0\n",
    "        else:\n",
    "            sup_score = 0.0\n",
    "        rel_score = 0.0\n",
    "        if self.rel_tokens is not None:\n",
    "            num = 0\n",
    "            for tok_idx, tok in enumerate(response.token_ids):\n",
    "                if tok in list(self.rel_tokens.values()):\n",
    "                    token = self.tokenizer.convert_ids_to_tokens(tok)\n",
    "                    if token == \"[Relevant]\":\n",
    "                        rel_score += response.logprobs[tok_idx][tok]\n",
    "                    num += 1\n",
    "            if num != 0:\n",
    "                rel_score /= num\n",
    "            else:\n",
    "                rel_score = 0.0\n",
    "        else:\n",
    "            rel_score = 0.0\n",
    "        use_score = 0.0\n",
    "        if self.use_tokens is not None:\n",
    "            num = 0\n",
    "            for tok_idx, tok in enumerate(response.token_ids):\n",
    "                if tok in list(self.use_tokens.values()):\n",
    "                    token = self.tokenizer.convert_ids_to_tokens(tok)\n",
    "                    if token == \"[Utility:1]\":\n",
    "                        use_score += response.logprobs[tok_idx][tok] * 0.2\n",
    "                    elif token == \"[Utility:2]\":\n",
    "                        use_score += response.logprobs[tok_idx][tok] * 0.2 + 0.2\n",
    "                    elif token == \"[Utility:3]\":\n",
    "                        use_score += response.logprobs[tok_idx][tok] * 0.2 + 0.4\n",
    "                    elif token == \"[Utility:4]\":\n",
    "                        use_score += response.logprobs[tok_idx][tok] * 0.2 + 0.6\n",
    "                    elif token == \"[Utility:5]\":\n",
    "                        use_score += response.logprobs[tok_idx][tok] * 0.2 + 0.8\n",
    "                    num += 1\n",
    "            if num != 0:\n",
    "                use_score /= num\n",
    "            else:\n",
    "                use_score = 0.0\n",
    "        else:\n",
    "            use_score = 0.0\n",
    "        if show_details:\n",
    "            print(f\"u:{use_score}, s:{sup_score}, r:{rel_score}\")\n",
    "        score = w_IsUSE * use_score + w_IsSUP * sup_score + w_IsREL * rel_score\n",
    "        return score\n",
    "    # 模型对话（面向用户）\n",
    "    # params\n",
    "    # query:str 输入\n",
    "    # return\n",
    "    # str 输出\n",
    "    def generate(self, query):\n",
    "        tmp_response = self.query_llm(self.format_prompt(query))\n",
    "        result = tmp_response.text\n",
    "        while(self.need_retrieve(tmp_response)):\n",
    "            documents = retriever.search(query + \"\\n\" + result, 3)\n",
    "            tmp_response = self.re_query(query + \"\\n\" + result, documents)\n",
    "            result += tmp_response.text\n",
    "        return result\n",
    "\n",
    "\n",
    "   \n",
    "            \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "local_model_path = \"self-rag/retrieval_lm/self-rag-model\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(local_model_path, padding_side=\"left\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, padding_side=\"left\")\n",
    "model = Model_LLM(local_model_path, 100, False, 10, tokenizer)\n",
    "\n",
    "# model = LLM(model=local_model_path, dtype=\"half\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b16eb-b0d8-47dd-9cfa-1f7dbcbc0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_prompt(input, paragraph=None):\n",
    "#     prompt = \"### Instruction:\\n{0}\\n\\n### Response:\\n\".format(input)\n",
    "#     if paragraph is not None:\n",
    "#         prompt += \"[Retrieval]<paragraph>{0}</paragraph>\".format(paragraph)\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5fb1c-4393-495f-b9a2-16befd7cc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(local_model_path, padding_side=\"left\")\n",
    "# sampling_params = SamplingParams(temperature=0.0, \n",
    "#                                       top_p=1.0, top_k = -1, max_tokens=100, skip_special_tokens=False, \n",
    "#                                       logprobs = 10, stop = [\"[Retrieval]\"])\n",
    "# ret_tokens, rel_tokens, sup_tokens, use_tokens = load_special_tokens(tokenizer, use_grounding=use_IsSUP, use_utility=use_IsUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e96bf4-aaea-4e75-8fce-79b5090d78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def need_retrieve(response: Response_LLM):\n",
    "#     if 32001 in response.token_ids:\n",
    "#         return True\n",
    "#     return False\n",
    "# # prompt:输入：list<str>\n",
    "# def query_llm(model, sampling_params, query):\n",
    "#     prompt = [query]\n",
    "#     print(f\"查询内容{prompt}\")\n",
    "#     preds = model.generate(prompt, sampling_params)\n",
    "#     pred_token_ids = preds[0].outputs[0].token_ids\n",
    "#     pred_text = preds[0].outputs[0].text\n",
    "#     pred_log_probs = []\n",
    "#     for logprob in preds[0].outputs[0].logprobs:\n",
    "#         tmp_log_probs = {}\n",
    "#         for key, value in logprob.items():\n",
    "#             tmp_log_probs[key] = np.exp(float(value))\n",
    "#         pred_log_probs.append(tmp_log_probs)\n",
    "#     response =  Response_LLM(pred_token_ids, pred_text, pred_log_probs)\n",
    "#     return response\n",
    "# # params\n",
    "# # query:之前查询的问题+之前生成的文本（去除特殊字符 TODO）\n",
    "# def re_query(model, sampling_params, prompt, documents):\n",
    "#     max_score = 0\n",
    "#     best_response = None\n",
    "#     for document in documents:\n",
    "#         response = query_llm(model, sampling_params, format_prompt(prompt, document))\n",
    "#         cur_score = eval_generation(response)\n",
    "#         print(\"此次检索的结果如下\")\n",
    "#         print(cur_score)\n",
    "#         print(response.token_ids)\n",
    "#         print(response.text)\n",
    "#         if (cur_score > max_score):\n",
    "#             max_score = cur_score\n",
    "#             best_response = response\n",
    "#     return best_response\n",
    "\n",
    "# def eval_generation(response:Response_LLM):\n",
    "#     sup_score = 0.0\n",
    "#     if sup_tokens is not None:\n",
    "#         num = 0\n",
    "#         for tok_idx, tok in enumerate(response.token_ids):\n",
    "#             if tok in list(sup_tokens.values()):\n",
    "#                 token = tokenizer.convert_ids_to_tokens(tok)\n",
    "#                 if token == \"[Fully supported]\":\n",
    "#                     sup_score += response.logprobs[tok_idx][tok] * 0.5 + 0.5\n",
    "#                 if token == \"[Partially supported]\":\n",
    "#                     sup_score += response.logprobs[tok_idx][tok] * 0.5\n",
    "#                 num += 1\n",
    "#         if num != 0:\n",
    "#             sup_score /= num\n",
    "#         else:\n",
    "#             sup_score = 0.0\n",
    "#     else:\n",
    "#         sup_score = 0.0\n",
    "#     rel_score = 0.0\n",
    "#     if rel_tokens is not None:\n",
    "#         num = 0\n",
    "#         for tok_idx, tok in enumerate(response.token_ids):\n",
    "#             if tok in list(rel_tokens.values()):\n",
    "#                 token = tokenizer.convert_ids_to_tokens(tok)\n",
    "#                 if token == \"[Relevant]\":\n",
    "#                     rel_score += response.logprobs[tok_idx][tok]\n",
    "#                 num += 1\n",
    "#         if num != 0:\n",
    "#             rel_score /= num\n",
    "#         else:\n",
    "#             rel_score = 0.0\n",
    "#     else:\n",
    "#         rel_score = 0.0\n",
    "#     use_score = 0.0\n",
    "#     if use_tokens is not None:\n",
    "#         num = 0\n",
    "#         for tok_idx, tok in enumerate(response.token_ids):\n",
    "#             if tok in list(use_tokens.values()):\n",
    "#                 token = tokenizer.convert_ids_to_tokens(tok)\n",
    "#                 if token == \"[Utility:1]\":\n",
    "#                     use_score += response.logprobs[tok_idx][tok] * 0.2\n",
    "#                 elif token == \"[Utility:2]\":\n",
    "#                     use_score += response.logprobs[tok_idx][tok] * 0.2 + 0.2\n",
    "#                 elif token == \"[Utility:3]\":\n",
    "#                     use_score += response.logprobs[tok_idx][tok] * 0.2 + 0.4\n",
    "#                 elif token == \"[Utility:4]\":\n",
    "#                     use_score += response.logprobs[tok_idx][tok] * 0.2 + 0.6\n",
    "#                 elif token == \"[Utility:5]\":\n",
    "#                     use_score += response.logprobs[tok_idx][tok] * 0.2 + 0.8\n",
    "#                 num += 1\n",
    "#         if num != 0:\n",
    "#             use_score /= num\n",
    "#         else:\n",
    "#             use_score = 0.0\n",
    "#     else:\n",
    "#         use_score = 0.0\n",
    "#     print(f\"u:{use_score}, s:{sup_score}, r:{rel_score}\")\n",
    "#     score = w_IsUSE * use_score + w_IsSUP * sup_score + w_IsREL * rel_score\n",
    "#     return score\n",
    "# def generate(model, sampling_params, query):\n",
    "#     tmp_response = query_llm(model, sampling_params, format_prompt(query))\n",
    "#     result = tmp_response.text\n",
    "#     while(need_retrieve(tmp_response)):\n",
    "#         documents = retriever.search(query + \"\\n\" + result, 3)\n",
    "#         tmp_response = re_query(model, sampling_params, query + \"\\n\" + result, documents)\n",
    "#         result += tmp_response.text\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf497cf-aa4a-4616-8adf-aed82486e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me the difference between llamas and alpacas?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7805e8-1979-4e15-9256-9c7e1c741d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# response = generate(model, sampling_params, query)\n",
    "response = model.generate(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6c406-9002-4215-a1bc-71644d2e182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b32a2-5c8d-4d7b-b922-4341c6e1cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464306f-37cd-475d-9b0c-e04e8e7ca30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = query + response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85838811-d1e3-4a13-bc34-415115cac32b",
   "metadata": {},
   "source": [
    "导入查询器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aac54f6-c128-4c32-a471-951bff2b2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from passage_retrieval import Retriever\n",
    "\n",
    "# class Retriever_LLM:\n",
    "#     def __init__(self, top_k:int = 5):\n",
    "#         self.retriever = Retriever({})\n",
    "#         self.retriever.setup_retriever_demo(\"self-rag/retrieval_lm/contriever\", \"self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_2020_dec_intro_only.jsonl\", \"self-rag/retrieval_lm/enwiki_2020_intro_only/enwiki_dec_2020_contriever_intro/*\",  n_docs=5, save_or_load_index=False)\n",
    "#     def search(self, query, n_docs:int = 5):\n",
    "#         retrieved_documents = self.retriever.search_document_demo(query, n_docs)\n",
    "#         return retrieved_documents\n",
    "\n",
    "# retriever = Retriever_LLM(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da89e210-777c-4159-a2a1-dfa57a50b938",
   "metadata": {},
   "source": [
    "将查询结果加入prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfba7ff-0caf-44de-b2c2-fc9a16d54833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def append_prompt(prompt:str, docs):\n",
    "#     prompts = [format_prompt(prompt, doc[\"title\"] +\"\\n\"+ doc[\"text\"]) for doc in docs]\n",
    "#     return prompts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fa094-bc10-45f8-adcf-08c8bc2f917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = retriever.search(query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc10e5-5c56-4b8b-a32a-0803844e4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = append_prompt(response.text, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2ea7a-69ec-46b5-97ce-abe18c507492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8adfb-45fb-4a3d-9205-0cd295b2001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [format_prompt(query_3, doc[\"title\"] +\"\\n\"+ doc[\"text\"]) for doc in retrieved_documents]\n",
    "# preds = model.generate(prompts, sampling_params)\n",
    "# top_doc = retriever.search_document_demo(query_3, 1)[0]\n",
    "# print(\"Reference: {0}\\nModel prediction: {1}\".format(top_doc[\"title\"] + \"\\n\" + top_doc[\"text\"], preds[0].outputs[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d7253-77b5-4a56-bed7-f26c35c90aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_tokens(response):\n",
    "#     token_dict = {}\n",
    "    \n",
    "#     for i, key in enumerate(response.token_ids):\n",
    "#         key = tokenizer.convert_ids_to_tokens(id)\n",
    "#         logprobs = sorted(response.logprobs[0], reverse=True)\n",
    "#         token_dict[key] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd2383-64f1-4cf8-9d4c-8ea8ef32dd85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59412c-8a72-4bfa-8cae-a2fdefd18c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def _relevance_score(pred_log_probs) -> float:\n",
    "#     rel_prob = np.exp(float(pred_log_probs[\"[Relevant]\"]))\n",
    "#     irel_prob = np.exp(float(pred_log_probs[\"[Irrelevant]\"]))\n",
    "#     return rel_prob / (rel_prob + irel_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddd0f5-fb5d-400d-9931-01f2c2ff9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 转化为id\n",
    "# text = '[Retrieval]'\n",
    "# token_ids = tokenizer.encode(text, max_length = 30, add_special_tokens = True, padding = 'max_length', truncation = True)\n",
    "# # [101, 791, 1921, 3221, 702, 1962, 1921, 3698, 8024, 2769, 812, 1377, 809, 1139, 1343, 6624, 6624, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    " \n",
    "# tokened_text = tokenizer.convert_ids_to_tokens(32000)\n",
    "# # ['[CLS]', '今', '天', '是', '个', '好', '天', '气', '，', '我', '们', '可', '以', '出', '去', '走', '走', '。', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf12382-e060-4d71-981d-5f71d5c927d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(token_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c53482-ecde-4514-b5d5-09206c9d5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokened_text = tokenizer.convert_ids_to_tokens(32000)\n",
    "# print(tokened_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62dfa80-2b8b-43d9-8df6-f63a2ba7d532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
